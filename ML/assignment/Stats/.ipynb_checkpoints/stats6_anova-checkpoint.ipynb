{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact the validity of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions Required to Use ANOVA\n",
    "\n",
    "Analysis of Variance (ANOVA) is a statistical technique used to compare means across multiple groups. For the results of an ANOVA to be valid, certain assumptions must be met:\n",
    "\n",
    "1. **Independence of Observations**:\n",
    "   - Each observation should be independent of others.\n",
    "   - **Violation Example**: In a study where participants are measured multiple times, the observations are not independent.\n",
    "\n",
    "2. **Normality**:\n",
    "   - The data in each group should be approximately normally distributed.\n",
    "   - **Violation Example**: If the data in one group is heavily skewed, the normality assumption is violated.\n",
    "\n",
    "3. **Homogeneity of Variances (Homoscedasticity)**:\n",
    "   - The variances among the groups should be approximately equal.\n",
    "   - **Violation Example**: If one group's variance is much larger or smaller than the others, this assumption is violated.\n",
    "\n",
    "4. **Random Sampling**:\n",
    "   - The data should be collected from a random sample of the population.\n",
    "   - **Violation Example**: If a convenience sample is used instead of a random sample, this assumption is violated.\n",
    "\n",
    "### Examples of Violations and Their Impact\n",
    "\n",
    "1. **Independence of Observations**:\n",
    "   - **Example**: In a classroom study where students' test scores are used, scores of students from the same group or classroom may not be independent.\n",
    "   - **Impact**: Violation of this assumption can lead to incorrect conclusions because the ANOVA test assumes that each data point is independent.\n",
    "\n",
    "2. **Normality**:\n",
    "   - **Example**: In a medical study measuring blood pressure, if the data for one group is highly skewed due to an outlier, this assumption is violated.\n",
    "   - **Impact**: Non-normal data can affect the Type I error rate, making it more likely to incorrectly reject the null hypothesis.\n",
    "\n",
    "3. **Homogeneity of Variances**:\n",
    "   - **Example**: In an educational study comparing test scores across different schools, if one school has much more variability in scores than others, this assumption is violated.\n",
    "   - **Impact**: Unequal variances can lead to an increased Type I error rate, affecting the reliability of the test results.\n",
    "\n",
    "4. **Random Sampling**:\n",
    "   - **Example**: In a market research study, if participants are selected based on convenience rather than randomly, this assumption is violated.\n",
    "   - **Impact**: Non-random sampling can introduce bias, making it difficult to generalize the results to the broader population.\n",
    "\n",
    "### How to Check for Assumptions\n",
    "\n",
    "1. **Independence**:\n",
    "   - This is usually determined by the study design.\n",
    "   - Ensure the data collection process is structured to avoid dependency among observations.\n",
    "\n",
    "2. **Normality**:\n",
    "   - Use graphical methods such as Q-Q plots or statistical tests like the Shapiro-Wilk test to check for normality.\n",
    "\n",
    "3. **Homogeneity of Variances**:\n",
    "   - Use Levene's test or Bartlett's test to assess the equality of variances.\n",
    "\n",
    "4. **Random Sampling**:\n",
    "   - Ensure the sampling method used is truly random and representative of the population.\n",
    "\n",
    "### What to Do if Assumptions are Violated\n",
    "\n",
    "1. **Independence**:\n",
    "   - Use a different statistical method that accounts for dependencies, such as mixed-effects models.\n",
    "\n",
    "2. **Normality**:\n",
    "   - Transform the data (e.g., log transformation) or use non-parametric tests like the Kruskal-Wallis test.\n",
    "\n",
    "3. **Homogeneity of Variances**:\n",
    "   - Use a different test such as Welch's ANOVA, which does not assume equal variances.\n",
    "\n",
    "4. **Random Sampling**:\n",
    "   - Improve the sampling method or use caution when interpreting the results, acknowledging the potential bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. What are the three types of ANOVA, and in what situations would each be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of ANOVA and Their Uses\n",
    "\n",
    "### Main Points:\n",
    "\n",
    "1. **One-Way ANOVA**\n",
    "   - Compares means of three or more independent groups based on one independent variable.\n",
    "   - Used when there is one categorical independent variable with three or more levels (groups) and one continuous dependent variable.\n",
    "   - Example: Comparing test scores of students from different teaching methods.\n",
    "\n",
    "2. **Two-Way ANOVA**\n",
    "   - Compares means of groups based on two independent variables and assesses the interaction effect between them.\n",
    "   - Used when there are two categorical independent variables and one continuous dependent variable.\n",
    "   - Example: Studying effects of different diets and exercise routines on weight loss.\n",
    "\n",
    "3. **Repeated Measures ANOVA**\n",
    "   - Used when the same subjects are measured multiple times under different conditions.\n",
    "   - Used when there is one categorical independent variable with repeated measures and one continuous dependent variable.\n",
    "   - Example: Measuring blood pressure of patients at different times after administering a drug.\n",
    "\n",
    "### Summary Table:\n",
    "\n",
    "| Type of ANOVA          | Number of Independent Variables | Situation Example                             |\n",
    "|------------------------|---------------------------------|----------------------------------------------|\n",
    "| One-Way ANOVA          | 1                               | Comparing test scores from different teaching methods |\n",
    "| Two-Way ANOVA          | 2                               | Studying the effects of different diets and exercise routines on weight loss |\n",
    "| Repeated Measures ANOVA| 1 (with repeated measures)      | Measuring blood pressure of patients at different times after administering a drug |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning of Variance in ANOVA\n",
    "\n",
    "### What is Partitioning of Variance?\n",
    "\n",
    "Partitioning of variance in ANOVA involves breaking down the total variability in the data into different components attributable to different sources. This process helps in understanding how much of the total variability is explained by the factors being studied and how much is due to random error.\n",
    "\n",
    "### Components of Variance\n",
    "\n",
    "1. **Total Sum of Squares (SST)**\n",
    "   - Represents the total variability in the data.\n",
    "   - Calculated as the sum of the squared differences between each observation and the overall mean.\n",
    "   - Formula: \n",
    "     \\[\n",
    "     $\\text{SST} = \\sum_{i=1}^{N} (X_i - \\bar{X})^2$\n",
    "     \\]\n",
    "\n",
    "2. **Between-Group Sum of Squares (SSB)**\n",
    "   - Represents the variability due to the differences between group means.\n",
    "   - Calculated as the sum of the squared differences between each group mean and the overall mean, weighted by the number of observations in each group.\n",
    "   - Formula: \n",
    "     \\[\n",
    "     $\\text{SSB} = \\sum_{j=1}^{k} n_j (\\bar{X}_j - \\bar{X})^2$\n",
    "     \\]\n",
    "\n",
    "3. **Within-Group Sum of Squares (SSW)**\n",
    "   - Represents the variability within each group.\n",
    "   - Calculated as the sum of the squared differences between each observation and its respective group mean.\n",
    "   - Formula: \n",
    "     \\[\n",
    "     $\\text{SSW} = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (X_{ij} - \\bar{X}_j)^2$\n",
    "     \\]\n",
    "\n",
    "### Importance of Understanding Partitioning of Variance\n",
    "\n",
    "1. **Identifying Sources of Variability**\n",
    "   - Helps in determining how much of the total variability is due to differences between groups (explained variability) and how much is due to random error (unexplained variability).\n",
    "\n",
    "2. **Hypothesis Testing**\n",
    "   - The partitioning of variance is crucial for conducting hypothesis tests in ANOVA. It allows for the calculation of the F-statistic, which is used to test if the group means are significantly different.\n",
    "\n",
    "3. **Interpreting Results**\n",
    "   - Understanding the proportion of total variability explained by the factors being studied (effect size) can provide insights into the strength and importance of the factors.\n",
    "\n",
    "4. **Model Assessment**\n",
    "   - Helps in assessing the goodness of fit of the model. A large between-group sum of squares relative to the within-group sum of squares indicates that the model explains a significant portion of the variability in the data.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Total Sum of Squares (SST)**: Total variability in the data.\n",
    "- **Between-Group Sum of Squares (SSB)**: Variability due to differences between group means.\n",
    "- **Within-Group Sum of Squares (SSW)**: Variability within each group.\n",
    "\n",
    "Understanding the partitioning of variance is essential for conducting ANOVA, interpreting the results, and assessing the significance and impact of the factors being studied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python, you can follow these steps:\n",
    "\n",
    "- Total Sum of Squares (SST): Measures the total variation in the data.\n",
    "- Explained Sum of Squares (SSE): Measures the variation explained by the groups (also known as sum of squares between groups).\n",
    "- Residual Sum of Squares (SSR): Measures the variation within the groups (also known as sum of squares within groups)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 443.33333333333337\n",
      "Explained Sum of Squares (SSE): 298.13333333333355\n",
      "Residual Sum of Squares (SSR): 145.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data for three groups\n",
    "group1 = np.array([23, 25, 27, 30, 22])\n",
    "group2 = np.array([31, 29, 27, 36, 38])\n",
    "group3 = np.array([19, 20, 23, 21, 24])\n",
    "\n",
    "# Combine all groups into a single array\n",
    "all_groups = np.concatenate([group1, group2, group3])\n",
    "\n",
    "# Overall mean\n",
    "overall_mean = np.mean(all_groups)\n",
    "\n",
    "# Group means\n",
    "mean_group1 = np.mean(group1)\n",
    "mean_group2 = np.mean(group2)\n",
    "mean_group3 = np.mean(group3)\n",
    "\n",
    "# Total sum of squares (SST)\n",
    "sst = np.sum((all_groups - overall_mean) ** 2)\n",
    "\n",
    "# Explained sum of squares (SSE)\n",
    "sse = (len(group1) * (mean_group1 - overall_mean) ** 2 +\n",
    "       len(group2) * (mean_group2 - overall_mean) ** 2 +\n",
    "       len(group3) * (mean_group3 - overall_mean) ** 2)\n",
    "\n",
    "# Residual sum of squares (SSR)\n",
    "ssr = (np.sum((group1 - mean_group1) ** 2) +\n",
    "       np.sum((group2 - mean_group2) ** 2) +\n",
    "       np.sum((group3 - mean_group3) ** 2))\n",
    "\n",
    "print(f'Total Sum of Squares (SST): {sst}')\n",
    "print(f'Explained Sum of Squares (SSE): {sse}')\n",
    "print(f'Residual Sum of Squares (SSR): {ssr}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Way ANOVA: Calculating Main Effects and Interaction Effects Using Python\n",
    "\n",
    "In a two-way ANOVA, the goal is to assess the main effects of two independent variables and their interaction effect on a dependent variable. Here's how you can calculate these effects using Python:\n",
    "\n",
    "1. **Main Effects**: The effect of each independent variable on the dependent variable.\n",
    "2. **Interaction Effect**: The combined effect of the two independent variables on the dependent variable.\n",
    "\n",
    "We'll use the `statsmodels` library, which provides tools for conducting ANOVA.\n",
    "\n",
    "First, install the `statsmodels` library if you haven't already:\n",
    "\n",
    "```bash\n",
    "pip install statsmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       sum_sq    df          F        PR(>F)\n",
      "C(FactorA)                0.2   1.0   0.058394  8.121220e-01\n",
      "C(FactorB)              320.0   1.0  93.430657  4.397672e-08\n",
      "C(FactorA):C(FactorB)     3.2   1.0   0.934307  3.481307e-01\n",
      "Residual                 54.8  16.0        NaN           NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Sample data\n",
    "# Suppose we have two factors: Factor A (with levels A1, A2) and Factor B (with levels B1, B2)\n",
    "# and a dependent variable (DV)\n",
    "data = {\n",
    "    'FactorA': np.repeat(['A1', 'A2'], 10),\n",
    "    'FactorB': np.tile(np.repeat(['B1', 'B2'], 5), 2),\n",
    "    'DV': [10, 15, 14, 10, 12, 20, 23, 21, 19, 22, 12, 14, 13, 11, 16, 21, 22, 20, 18, 21]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fit the two-way ANOVA model\n",
    "model = ols('DV ~ C(FactorA) + C(FactorB) + C(FactorA):C(FactorB)', data=df).fit()\n",
    "\n",
    "# Perform the ANOVA\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(anova_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "#### Data Preparation:\n",
    "\n",
    "We create a dataset with two factors (FactorA and FactorB) and a dependent variable (DV).\n",
    "#### Model Fitting:\n",
    "\n",
    "We use the ols function to fit an ordinary least squares regression model. The formula 'DV ~ C(FactorA) + C(FactorB) + C(FactorA):C(FactorB)' specifies that we want to include main effects for FactorA and FactorB, as well as their interaction effect.\n",
    "#### ANOVA Table:\n",
    "\n",
    "We use sm.stats.anova_lm(model, typ=2) to perform the ANOVA and get the ANOVA table.\n",
    "### Output:\n",
    " The ANOVA table will contain the following columns:\n",
    "- sum_sq: Sum of squares for each source of variation.\n",
    "- df: Degrees of freedom for each source of variation.\n",
    "- F: F-statistic for each source of variation.\n",
    "- PR(>F): p-value for the F-statistic.\n",
    "This table provides information about the main effects of FactorA and FactorB, as well as their interaction effect. The significance of each effect can be determined by examining the p-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02. What can you conclude about the differences between the groups, and how would you interpret these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of One-Way ANOVA Results\n",
    "\n",
    "Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02. Here's how you can interpret these results:\n",
    "\n",
    "### Hypotheses in One-Way ANOVA\n",
    "\n",
    "- **Null Hypothesis ($H_0$)**: The means of all groups are equal. There are no differences between the groups.\n",
    "- **Alternative Hypothesis ($ H_a $)**: At least one group mean is different from the others.\n",
    "\n",
    "### F-Statistic and P-Value\n",
    "\n",
    "- **F-Statistic (5.23)**: This value indicates the ratio of the variance between the group means to the variance within the groups. A higher F-statistic suggests greater variability between group means compared to within-group variability.\n",
    "- **P-Value (0.02)**: The p-value indicates the probability of observing an F-statistic as extreme as 5.23, assuming the null hypothesis is true. A lower p-value suggests stronger evidence against the null hypothesis.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **Significance Level ($\\alpha$)**: Commonly, a significance level of 0.05 is used.\n",
    "- **P-Value Comparison**: The obtained p-value (0.02) is less than the significance level (0.05).\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Since the p-value (0.02) is less than 0.05, we reject the null hypothesis. This means there is statistically significant evidence to conclude that there are differences between the group means. In other words, at least one group mean is significantly different from the others.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Post-Hoc Tests**: To determine which specific groups differ from each other, you can perform post-hoc tests such as Tukey's HSD (Honestly Significant Difference) test.\n",
    "- **Effect Size**: Consider calculating the effect size (e.g., eta squared) to understand the magnitude of the differences between groups.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The one-way ANOVA results indicate that there are significant differences between the group means, given an F-statistic of 5.23 and a p-value of 0.02. Further analysis through post-hoc tests can help identify the specific groups that differ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential consequences of using different methods to handle missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data in Repeated Measures ANOVA\r\n",
    "\r\n",
    "In a repeated measures ANOVA, missing data can be a challenge because the same subjects are measured under different conditions or over time. Handling missing data appropriately is crucial to maintaining the integrity of the analysis.\r\n",
    "\r\n",
    "### Methods to Handle Missing Data\r\n",
    "\r\n",
    "1. **Listwise Deletion (Complete Case Analysis)**\r\n",
    "    - **Description**: Exclude any subject with missing data from the analysis.\r\n",
    "    - **Pros**: Simple to implement; maintains consistency in sample size across measurements.\r\n",
    "    - **Cons**: Reduces sample size and statistical power; can introduce bias if the missing data are not completely random.\r\n",
    "\r\n",
    "2. **Pairwise Deletion**\r\n",
    "    - **Description**: Use all available data by excluding missing data on a case-by-case basis.\r\n",
    "    - **Pros**: Uses more data compared to listwise deletion; can be more efficient.\r\n",
    "    - **Cons**: Can lead to inconsistencies in the analysis; more complex to implement and interpret.\r\n",
    "\r\n",
    "3. **Mean Imputation**\r\n",
    "    - **Description**: Replace missing values with the mean of the observed values for that variable.\r\n",
    "    - **Pros**: Simple to implement; retains all subjects in the analysis.\r\n",
    "    - **Cons**: Underestimates variability; can bias parameter estimates downward.\r\n",
    "\r\n",
    "4. **Last Observation Carried Forward (LOCF)**\r\n",
    "    - **Description**: Replace missing values with the last observed value for that subject.\r\n",
    "    - **Pros**: Retains all subjects in the analysis; useful in longitudinal studies.\r\n",
    "    - **Cons**: Assumes stability of the variable over time; can bias results if this assumption is not met.\r\n",
    "\r\n",
    "5. **Multiple Imputation**\r\n",
    "    - **Description**: Replace missing values with multiple sets of simulated values to create several complete datasets, analyze each one, and then combine the results.\r\n",
    "    - **Pros**: Accounts for uncertainty in the imputation process; provides more accurate standard errors and confidence intervals.\r\n",
    "    - **Cons**: Computationally intensive; requires more advanced statistical knowledge and software.\r\n",
    "\r\n",
    "6. **Maximum Likelihood Estimation (MLE)**\r\n",
    "    - **Description**: Use all available data to estimate the model parameters directly.\r\n",
    "    - **Pros**: Efficient use of all data; provides unbiased estimates under the assumption of missing at random (MAR).\r\n",
    "    - **Cons**: Requires sophisticated software and understanding of likelihood-based methods.\r\n",
    "\r\n",
    "### Potential Consequences of Different Methods\r\n",
    "\r\n",
    "1. **Listwise Deletion**: Reduces the sample size, which can lead to a loss of statistical power and potentially biased results if the data are not missing completely at random (MCAR).\r\n",
    "\r\n",
    "2. **Pairwise Deletion**: Can introduce inconsistencies in sample size across different analyses, complicating interpretation and possibly leading to biased results.\r\n",
    "\r\n",
    "3. **Mean Imputation**: Reduces variability and can bias parameter estimates downward, potentially leading to incorrect conclusions.\r\n",
    "\r\n",
    "4. **LOCF**: Assumes no change over time, which can lead to biased results if the assumption is incorrect. It can also underestimate variability.\r\n",
    "\r\n",
    "5. **Multiple Imputation**: More accurate and robust, but computationally intensive and requires more advanced statistical knowledge.\r\n",
    "\r\n",
    "6. **Maximum Likelihood Estimation**: Efficient and unbiased under the MAR assumption but requires sophisticated software and statistical understanding.\r\n",
    "\r\n",
    "### Recommendations\r\n",
    "\r\n",
    "- **Assess Missing Data Mechanism**: Before choosing a method, assess the pattern and mechanism of the missing data (e.g., MCAR, MAR, or not missing at random (NMAR)).\r\n",
    "- **Use Advanced Methods**: If possible, prefer multiple imputation or MLE, as they provide more accurate and reliable results by accounting for the uncertainty in the missing data.\r\n",
    "- **Sensitivity Analysis**: Perform sensitivity analyses to check how different methods of handling missing data affect the results.\r\n",
    "\r\n",
    "### Summary\r\n",
    "\r\n",
    "Handling missing data in repeated measures ANOVA is crucial for maintaining the validity of the analysis. Different methods have various pros and cons, and the choice of method can significantly impact the results. Advanced methods like multiple imputation or maximum likelihood estimation are generally preferred for their accuracy and robustness.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide \n",
    "an example of a situation where a post-hoc test might be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Post-Hoc Tests Used After ANOVA\r\n",
    "\r\n",
    "Post-hoc tests are performed after an ANOVA when the null hypothesis is rejected, indicating that at least one group mean is different from the others. These tests help determine which specific groups differ from each other. Here are some common post-hoc tests and when to use them:\r\n",
    "\r\n",
    "### 1. Tukey's Honestly Significant Difference (HSD) Test\r\n",
    "- **Use Case**: When you want to compare all possible pairs of group means.\r\n",
    "- **Assumptions**: Equal variances and sample sizes, although robust to minor deviations.\r\n",
    "- **Example**: Comparing the effectiveness of different teaching methods (e.g., traditional, online, hybrid) on student performance.\r\n",
    "\r\n",
    "### 2. Bonferroni Correction\r\n",
    "- **Use Case**: When you want to control the family-wise error rate by adjusting the significance level.\r\n",
    "- **Assumptions**: Same as the original ANOVA test.\r\n",
    "- **Example**: Multiple comparisons in a clinical trial where several treatments are tested against a control.\r\n",
    "\r\n",
    "### 3. Scheffé's Method\r\n",
    "- **Use Case**: When you need a more conservative test that can handle unequal sample sizes and variances.\r\n",
    "- **Assumptions**: Suitable for complex comparisons involving linear combinations of group means.\r\n",
    "- **Example**: Comparing different diet plans on weight loss where variances and sample sizes might differ.\r\n",
    "\r\n",
    "### 4. Dunnett's Test\r\n",
    "- **Use Case**: When comparing multiple treatment groups to a single control group.\r\n",
    "- **Assumptions**: Assumes equal variances across groups.\r\n",
    "- **Example**: Testing new drug formulations against a standard control drug.\r\n",
    "\r\n",
    "### 5. Holm's Sequential Bonferroni Procedure\r\n",
    "- **Use Case**: When you need a stepwise method to control the family-wise error rate.\r\n",
    "- **Assumptions**: Similar to the Bonferroni correction.\r\n",
    "- **Example**: Comparing different fertilizer types on plant growth.\r\n",
    "\r\n",
    "### 6. Fisher's Least Significant Difference (LSD) Test\r\n",
    "- **Use Case**: When you want to perform multiple comparisons without adjusting for multiple testing (more liberal approach).\r\n",
    "- **Assumptions**: Equal variances and normally distributed errors.\r\n",
    "- **Example**: Initial screening of potential factors affecting product quality in a manufacturing process.\r\n",
    "\r\n",
    "### Example Situation\r\n",
    "\r\n",
    "Suppose you conducted an ANOVA to evaluate the effectiveness of different study techniques (Group A: Flashcards, Group B: Highlighting, Group C: Summarization, Group D: Rereading) on exam scores. The ANOVA results show a significant difference between the groups (p-value < 0.05). To determine which specific groups differ, you can perform a post-hoc test.\r\n",
    "\r\n",
    "### Performing Tukey's HSD Test in Python\r\n",
    "\r\n",
    "Here's how you can ups=df['StudyTechnique'], alpha=0.05)\r\n",
    "\r\n",
    "print(tukey_results)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Multiple Comparison of Means - Tukey HSD, FWER=0.05        \n",
      "==================================================================\n",
      "   group1        group2    meandiff p-adj   lower    upper  reject\n",
      "------------------------------------------------------------------\n",
      "  Flashcards  Highlighting     -4.6 0.0026  -7.8319 -1.3681   True\n",
      "  Flashcards     Rereading    -10.4    0.0 -13.6319 -7.1681   True\n",
      "  Flashcards Summarization      4.0 0.0103   0.7681  7.2319   True\n",
      "Highlighting     Rereading     -5.8 0.0001  -9.0319 -2.5681   True\n",
      "Highlighting Summarization      8.6    0.0   5.3681 11.8319   True\n",
      "   Rereading Summarization     14.4    0.0  11.1681 17.6319   True\n",
      "------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'StudyTechnique': np.repeat(['Flashcards', 'Highlighting', 'Summarization', 'Rereading'], 10),\n",
    "    'ExamScore': [78, 85, 84, 79, 88, 85, 82, 86, 87, 84, 75, 73, 78, 77, 80, 79, 82, 81, 83, 84,\n",
    "                  88, 85, 87, 86, 90, 88, 87, 89, 88, 90, 72, 70, 75, 74, 73, 75, 74, 76, 73, 72]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform Tukey's HSD test\n",
    "tukey_results = pairwise_tukeyhsd(endog=df['ExamScore'], groups=df['StudyTechnique'], alpha=0.05)\n",
    "\n",
    "print(tukey_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from \n",
    "50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Pytho \r\n",
    "to determine if there are any significant differences between the mean weight loss of the three diet .\r\n",
    "Report the F-statistic and p-value, and interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Way ANOVA: Comparing Mean Weight Loss of Three Diets\r\n",
    "\r\n",
    "A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from 50 participants who were randomly assigned to one of the diets. We'll conduct a one-way ANOVA using Python to determine if there are any significant differences between the mean weight loss of the three diets.\r\n",
    "\r\n",
    "### Step-by-Step Process\r\n",
    "\r\n",
    "1. **Import Libraries**: We'll use `pandas`, `numpy`, and `statsmodels` for the analysis.\r\n",
    "2. **Prepare Data**: Create a dataset with the weight loss data for the three diets.\r\n",
    "3. **Fit ANOVA Model**: Use `statsmodels` to fit the ANOVA model.\r\n",
    "4. **Perform ANOVA**: Get the ANOVA table and extract the F-statistic and p-value.\r\n",
    "5. **Interpret Results**: Determine if there are significant differences between the diets.\r\n",
    "\r\n",
    "### int the ANOVA table\r\n",
    "print(anova_table)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              sum_sq    df         F    PR(>F)\n",
      "C(Diet)    37.529415   2.0  7.175732  0.001907\n",
      "Residual  122.906107  47.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Sample data\n",
    "np.random.seed(0)\n",
    "diet_A = np.random.normal(loc=5, scale=1.5, size=17)  # Mean weight loss = 5, SD = 1.5\n",
    "diet_B = np.random.normal(loc=6, scale=1.5, size=17)  # Mean weight loss = 6, SD = 1.5\n",
    "diet_C = np.random.normal(loc=4.5, scale=1.5, size=16)  # Mean weight loss = 4.5, SD = 1.5\n",
    "\n",
    "# Combine data into a DataFrame\n",
    "data = {\n",
    "    'WeightLoss': np.concatenate([diet_A, diet_B, diet_C]),\n",
    "    'Diet': ['A'] * 17 + ['B'] * 17 + ['C'] * 16\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fit the one-way ANOVA model\n",
    "model = ols('WeightLoss ~ C(Diet)', data=df).fit()\n",
    "\n",
    "# Perform the ANOVA\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Print the ANOVA table\n",
    "print(anova_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "The F-statistic is 7.1757, and the p-value is 0.0019. Since the p-value is less than the common significance level of 0.05, we reject the null hypothesis. This indicates that there are significant differences in mean weight loss among the three diets.\n",
    "\n",
    "### Conclusion\n",
    "The one-way ANOVA results suggest that the mean weight loss differs significantly between at least two of the diets (A, B, and C). To determine which specific diets differ, a post-hoc test such as Tukey's HSD can be performed.\n",
    "\n",
    "### Performing Post-Hoc Test (Tukey's HSD)\n",
    "To identify which specific diets differ, you can perform Tukey's HSD test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
      "====================================================\n",
      "group1 group2 meandiff p-adj   lower   upper  reject\n",
      "----------------------------------------------------\n",
      "     A      B  -0.1749 0.9467 -1.5172  1.1674  False\n",
      "     A      C  -1.9383 0.0034 -3.3014 -0.5751   True\n",
      "     B      C  -1.7634 0.0083 -3.1265 -0.4002   True\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Perform Tukey's HSD test\n",
    "tukey_results = pairwise_tukeyhsd(endog=df['WeightLoss'], groups=df['Diet'], alpha=0.05)\n",
    "\n",
    "print(tukey_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "By conducting a one-way ANOVA, we found significant differences in mean weight loss between the three diets. The F-statistic was 7.1757, and the p-value was 0.0019. A post-hoc test like Tukey's HSD can be used to identify which specific diets have different mean weight losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10. A company wants to know if there are any significant differences in the average time it takes to \n",
    "complete a task using three different software programs: Program A, Program B, and Program C. The \r\n",
    "randomly assign 30 employees to one of the programs and record the time it takes each employee  o\r\n",
    "complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or\r\n",
    "interaction effects between the software programs and employee experience level (novice vs.\r\n",
    "experienced). Report the F-statistics and p-values, and interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
